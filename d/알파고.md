**알파고**는 [구글](https://www.google.com/) 딥마인드(DeepMind Technologies)에서 개발한 인공지능 바둑 프로그램으로, 딥러닝과 몬테카를로 트리 탐색(MCTS)을 결합해 인간 최고 수준의 바둑 기사들을 상대로 승리한 최초의 AI이다. 2015년 최초 공개 이후 `AlphaGo Fan`, `AlphaGo Master`, `AlphaGo Zero` 등 여러 버전이 출시되었으며, 이후 `AlphaZero`, `MuZero` 등 일반화된 강화학습 시스템의 기반이 되었다.

***

# 📋 개요 (Infobox)

| 항목           | 내용                                                                                                                                                       |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **이름**       | 알파고 (AlphaGo)                                                                                                                                            |
| **개발자**      | [DeepMind](https://deepmind.google/) Technologies (구글 딥마인드)                                                                                                                          |
| **최초 공개**    | 2015년                                                                                                                                                    |
| **주요 버전**    | AlphaGo Fan (2015), AlphaGo Lee (2016), AlphaGo Master (2016), AlphaGo Zero (2017)                                                                       |
| **프로그래밍 언어** | C++, Python                                                                                                                                              |
| **운영 체제**    | Linux                                                                                                                                                    |
| **주요 기술**    | 딥 러닝 (CNN), 강화학습, 몬테카를로 트리 탐색 (MCTS)                                                                                                                     |
| **분야**       | 인공지능, 바둑                                                                                                                                                 |
| **라이선스**     | 비공개 (폐쇄형)                                                                                                                                                |
| **관련 논문**    | *Mastering the Game of Go with Deep Neural Networks and Tree Search* (Nature, 2016)¹, *Mastering the game of Go without human knowledge* (Nature, 2017)² |

***

# 1. 소개

알파고는 **딥러닝 기반 정책망(Policy Network)** 과 **가치망(Value Network)** 을 활용해 바둑 수읽기와 수읽기 평가를 수행하고, 이를 **몬테카를로 트리 탐색(MCTS)** 으로 결합해 최적의 수를 탐색한다. 기존 바둑 [AI](/d/인공지능)는 주로 인간 기보를 기반으로 한 규칙 기반 엔진에 의존했지만, 알파고는 대규모 **강화학습**과 **자기대국(self‑play)** 을 통해 스스로 전략을 학습한다.

* **정책망**: 현재 바둑판 상태에서 가능한 착점마다 확률을 예측하여 “어디에 두면 좋을까?”를 제시한다.

* **가치망**: 현재 판의 승률을 0\~1 사이의 값으로 추정한다.

* **MCTS**: 정책망이 제시한 후보 수를 바탕으로 시뮬레이션 트리를 확장하고, 가치망이 평가한 승률을 이용해 트리 노드의 가치를 업데이트한다.

이러한 구조는 바둑이라는 **극단적으로 큰 탐색 공간**(가능한 경우의 수가 10¹⁸⁰ 이상)에 효율적으로 대응할 수 있게 만든다.

***

# 2. 개발 배경 및 동기

* **바둑은 인간 지능의 정점**이라고 여겨지는 게임이며, 체스보다 훨씬 더 복잡한 상태 공간을 가진다. 딥마인드의 설립 목표 중 하나는 인간 수준 혹은 그 이상의 인공지능을 구현하는 것이었으며, 바둑은 그 도전 과제의 대표적인 사례였다.

* 2014년 딥마인드 연구원 **David Silver**와 **Demis Hassabis**가 주도한 프로젝트는 **딥 신경망**과 **강화학습**을 결합한 새로운 접근법을 시험하고자 시작되었다.

***

# 3. 주요 기술·아키텍처

| 구성 요소                             | 설명                                                                                                                                                     |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **정책망 (Policy Network)**          | 19×19 바둑판을 입력으로 받아 361개의 가능한 착점에 대한 확률 분포를 출력한다. 최초 버전은 **13계층 CNN**(Convolutional Neural Network)을 사용했으며, 이후 **Residual Network (ResNet)** 구조로 개선되었다. |
| **가치망 (Value Network)**           | 정책망과 동일한 입력을 받아 현재 판이 승리로 이어질 확률을 추정한다. 19×19 바둑판 전체를 한 번에 평가하여 MCTS에서 **시뮬레이션 수**를 크게 줄였다.                                                            |
| **몬테카를로 트리 탐색 (MCTS)**            | 정책망에서 제공된 후보 수를 기반으로 트리를 확장하고, 가치망이 제공한 승률을 통해 **UCB1**(Upper Confidence Bound)식으로 탐색‑활용 균형을 조절한다.                                                     |
| **강화학습 (Reinforcement Learning)** | `AlphaGo Zero`에서는 **전통적인 인간 기보 없이** 자체 대국을 통해 수백만 번의 게임을 반복하면서 네트워크 가중치를 업데이트한다. 이는 **Self‑play** 라고 불리며, 인간 지식 없이도 최적 전략을 스스로 학습한다.                   |
| **하드웨어**                          | 초기 AlphaGo는 \*\*GPU(GeForce GTX 980)\*\*와 \*\*TPU(다중 CPU 코어)\*\*를 활용해 훈련 및 추론을 수행했으며, `AlphaGo Zero`는 **구글 클라우드 TPU**를 이용해 훈련 속도를 크게 향상시켰다.            |

## 3.1 AlphaGo 버전별 주요 특징

| 버전                 | 발표 연도      | 주요 특징                                                                       | 주요 대전                        |
| ------------------ | ---------- | --------------------------------------------------------------------------- | ---------------------------- |
| **AlphaGo Fan**    | 2015       | 인간 기보 기반 초기 버전, 정책망과 가치망을 각각 별도로 훈련                                         | Fan Hui (5:0 승)              |
| **AlphaGo Lee**    | 2016       | 정책망 + 가치망 통합, MCTS와 결합, 인간 기보 + 자체 대국 혼합 학습                                 | 이세돌 9단 (4:1 승)               |
| **AlphaGo Master** | 2016 (온라인) | 60억 수의 자체 대국(셀프플레이) 결과를 반영, 네트워크 깊이 20+ 레이어                                 | 온라인 60:0 전승 (다양한 프로 기사)      |
| **AlphaGo Zero**   | 2017       | 인간 기보 전혀 사용 안 함, 완전 셀프플레이 기반, 파라미터 4배 감소, 3일에 걸친 훈련만으로 기존 AlphaGo Master 능가 | 자체 평가 (AlphaGo Master 대비 우위) |
| **AlphaZero**      | 2017       | 바둑·체스·쇼기 전반에 적용 가능한 일반 강화학습 프레임워크, 동일 알고리즘으로 4시간 내 최상위 수준 달성                | 체스·쇼기·바둑 모두에서 인간 최고 수준 초과    |

***

# 4. 대표적인 대국 및 성과

## 4.1 Lee Sedol (이세돌)과의 5대국 (2016)

| 경기      | 일자         | 승자            | 핵심 포인트                                           |
| ------- | ---------- | ------------- | ------------------------------------------------ |
| **제1국** | 2016‑03‑09 | 알파고 (이세돌 0:1) | 초반부터 강력한 중앙 장악                                   |
| **제2국** | 2016‑03‑10 | 알파고 (2:0)     | 전통적인 인간 전략보다 독창적인 ‘신동’ 수                         |
| **제3국** | 2016‑03‑12 | 알파고 (3:0)     | 복잡한 ‘대사변’ (대형 모양) 활용                             |
| **제4국** | 2016‑03‑13 | 알파고 (4:0)     | ‘교환’를 통한 장기 승리 기대                                |
| **제5국** | 2016‑03‑15 | 이세돌 (4:1)     | 78번째 수(흑 78수)에서 인간 수준을 넘어서는 ‘신선한 발상’ – 알파고의 첫 패배 |

> 5번째 대국에서 이세돌이 만든 **78번째 수**는 이후 수많은 바둑 기사와 AI 연구자에게 “인간 창의성의 상징”으로 회자된다. 알파고는 이 수에 대해 “예측이 불가능했다”고 공식 입장을 밝혔다⁽³⁾.

## 4.2 Ke Jie (구레이)와의 3대국 (2017)

| 경기      | 일자         | 승자            | 특징                              |
| ------- | ---------- | ------------- | ------------------------------- |
| **제1국** | 2017‑05‑23 | 알파고 (구레이 0:1) | Black‑handed 돌입 전략, 초반 ‘핵심 포석’  |
| **제2국** | 2017‑05‑24 | 알파고 (0:2)     | ‘세 번의 대세’를 이용한 압박 전술            |
| **제3국** | 2017‑05‑25 | 알파고 (0:3)     | ‘완전한 자가 학습’ 전략, 인간 기보와 전혀 다른 흐름 |

이 대국은 **AlphaGo Zero** 기반으로 진행되었으며, 인간 기보 없이 완전히 셀프플레이만으로 훈련된 모델이 인간 최고 수준을 압도한 사례로 기록된다⁽⁴⁾.

## 4.3 온라인 마스터 대전 (2016)

알파고는 2개월간 \*\*온라인 바둑 서버(GoKGS, Pandanet, Tygem 등)\*\*에서 60연승을 기록했다. 상대 기사들은 모두 **전문가(프로 기사) 또는 9단 이상** 수준이었으며, 승률이 \*\*99.9%\*\*에 육박했다.

***

# 5. 영향 및 유산

## 5.1 인공지능 연구에 미친 파급 효과

* **딥러닝 + 강화학습** 결합 모델이 **다양한 게임(체스, 쇼기, 스타크래프트 등)** 및 **정책 최적화** 분야에 적용되면서, `AlphaZero`와 `MuZero` 등 차세대 알고리즘이 등장했다.

* `AlphaGo`는 **전이 학습(transfer learning)** 및 **자기 지도 학습(self‑supervised learning)** 의 실용성을 입증했으며, **비전 분야**·**자연어 처리(NLP)**·**로보틱스** 등에서도 유사한 접근법이 채택되었다.

## 5.2 바둑 문화와 교육

* 알파고는 바둑 기사들에게 \*\*“새로운 수(새벽 전략)”\*\*를 제공해 전통적인 교본을 재해석하게 만들었다. 많은 프로 기사들이 알파고의 수를 학습 자료로 활용하고 있다.

* 알파고가 만든 **‘신동 수’**(특히 4번째 대국에서 나왔던 ‘신동(신규 독창) 수’)는 바둑 이론서에 새롭게 실리며, **‘인공지능이 인간 창의성을 넘을 수 있다’는 논쟁**을 촉발했다.

## 5.3 윤리·사회적 논의

* AI가 인간 전문가를 능가하면서 **직업 대체**에 대한 우려가 제기되었다.

* 알파고의 개발 과정에서 **데이터 소유권**, **연산 비용(전력 소모)**, **공정성** 등에 대한 논의가 활발히 이루어졌다.

***

# 6. 관련 기술 및 파생 프로젝트

| 프로젝트             | 핵심 아이디어                                              | 적용 분야                   |
| ---------------- | ---------------------------------------------------- | ----------------------- |
| **AlphaZero**    | 바둑·체스·쇼기 모두 동일한 강화학습 프레임워크 사용                        | 보드 게임 전반, 전략 시뮬레이션      |
| **MuZero**       | 환경 모델(transition dynamics)을 학습해 모델‑프리와 모델‑베이스의 장점 결합 | Atari, 체스, 쇼기, 바둑, 로보틱스 |
| **DeepMind Lab** | 3D 환경에서 강화학습을 적용한 일반 AI 연구 플랫폼                       | 인지 과학, 로보틱스             |
| **OpenAI Five**  | Dota 2와 같은 복잡한 실시간 전략 게임에서 멀티 에이전트 학습                | 멀티 에이전트 시스템             |
| **Leela Zero**   | 커뮤니티 기반 오픈소스 AlphaGo Zero 구현                         | 바둑 AI 커뮤니티, 오픈소스 연구     |

# 7. 참고 문헌

1. Silver, D., et al. “Mastering the game of Go with deep neural networks and tree search.” *Nature* 529, 484–489 (2016). DOI: 10.1038/nature16961.
2. Silver, D., et al. “Mastering the game of Go without human knowledge.” *Nature* 550, 354–359 (2017). DOI: 10.1038/nature24270.
3. DeepMind Blog. “The 78th move – why an AI can beat a world champion.” (2016년 5월). <https://deepmind.com/blog/article/78th-move>
4. DeepMind Blog. “AlphaGo Zero: Starting from scratch.” (2017년 12월). <https://deepmind.com/blog/article/alphago-zero-starting-scratch>
5. Lee, S. “고와 인공지능: 알파고가 바둑을 바꾸다.” *바둑학 연구* 32(3), 45‑68 (2018).
6. Hassabis, D., & Silver, D. “Artificial intelligence: The power of learning.” *Scientific American* (2020년 9월).

# 8. 외부 링크

* **DeepMind – AlphaGo 공식 페이지**: <https://deepmind.com/research/case-studies/alphago>
* **AlphaGo 논문 리스트 (arXiv)**: <https://arxiv.org/search/?query=alphago&searchtype=all>
* **AlphaGo Zero 오픈소스 구현 (Leela Zero)**: <https://github.com/leela-zero/leela-zero>
* **바둑 전문 커뮤니티(GoKGS) 알파고 대전 기록**: <https://www.gokgs.com/>

## 9. 관련 항목

* **딥러닝(Deep Learning)**
* **강화학습(Reinforcement Learning)**
* **몬테카를로 트리 탐색(MCTS)**
* **AlphaZero**
* **MuZero**
* **인공지능 윤리 (AI Ethics)**

*이 문서는 2025년 8월 기준 최신 정보를 바탕으로 작성되었습니다.*
