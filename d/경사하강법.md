# 경사하강법 (Gradient Descent)

> **경사하강법**(Gradient Descent)은 미분 가능한 함수의 최소값(또는 최대값)을 찾기 위해 함수의 기울기(gradient)를 이용해 반복적으로 파라미터를 업데이트하는 최적화 알고리즘이다. 머신러닝·딥러닝 모델의 가중치를 학습시키는 가장 기본적인 방법으로 널리 사용된다.

---

## 목차
1. [개요](#개요)  
2. [수학적 배경](#수학적-배경)  
3. [알고리즘 흐름](#알고리즘-흐름)  
4. [변형·확장](#변형확장)  
   - 4.1 [확률적 경사하강법 (SGD)](#확률적-경사하강법-sgd)  
   - 4.2 [미니배치 경사하강법](#미니배치-경사하강법)  
   - 4.3 [모멘텀](#모멘텀)  
   - 4.4 [AdaGrad, RMSprop, Adam 등 적응 학습률 방법](#적응-학습률-방법)  
5. [학습률 선택](#학습률-선택)  
6. [수렴 이론](#수렴-이론)  
7. [장점·단점](#장점단점)  
8. [응용 사례](#응용-사례)  
9. [구현 예시 (Python)](#구현-예시-python)  
10. [참고 문헌](#참고-문헌)  
11. [외부 링크](#외부-링크)  

---

## 개요
경사하강법은 다음과 같은 최적화 문제에 적용된다.

\[
\theta^{*} = \arg\min_{\theta} \; L(\theta)
\]

- \(L(\theta)\) : 파라미터 \(\theta\)에 대한 손실(목적) 함수  
- \(\theta\) : 모델 가중치·편향 등 학습 가능한 파라미터 집합  

핵심 아이디어는 **현재 파라미터 위치에서 손실 함수의 기울기(gradient)를 계산하고, 그 반대 방향(내리막길)으로 일정 단계만큼 이동**하는 것이다.  

---

## 수학적 배경
### 1. Gradient (기울기)
다변량 함수 \(L:\mathbb{R}^{n}\to\mathbb{R}\)의 기울기는 각 변수에 대한 편미분을 모은 벡터이다.

\[
\nabla_{\theta} L(\theta) = \left[\frac{\partial L}{\partial \theta_1},\dots,\frac{\partial L}{\partial \theta_n}\right]^{\!T}
\]

### 2. 업데이트 식
가장 기본적인 **배치 경사하강법(Batch Gradient Descent)** 은 다음과 같이 파라미터를 업데이트한다.

\[
\theta^{(t+1)} = \theta^{(t)} - \alpha \, \nabla_{\theta} L\big(\theta^{(t)}\big)
\]

- \(t\) : 현재 반복(Iteration) 번호  
- \(\alpha\) : **학습률(learning rate)**, 양의 실수이며 step size를 조절한다.

### 3. 1차 테일러 전개와 직관
함수 \(L\) 를 현재 파라미터 \(\theta^{(t)}\) 근처에서 1차 테일러 전개하면

\[
L(\theta^{(t+1)}) \approx L(\theta^{(t)}) + \nabla_{\theta}L(\theta^{(t)})^{\!T}(\theta^{(t+1)}-\theta^{(t)})
\]

\(\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_{\theta}L\) 를 대입하면

\[
L(\theta^{(t+1)}) \approx L(\theta^{(t)}) - \alpha \|\nabla_{\theta}L(\theta^{(t)})\|^{2}
\]

학습률 \(\alpha>0\) 이면 손실이 감소한다는 것을 보여준다(단, \(\alpha\)가 너무 크면 2차항 이상의 항 때문에 발산할 수 있음).

---

## 알고리즘 흐름
```text
Initialize θ (randomly or with heuristics)
repeat until convergence or max_iter:
    Compute gradient g = ∇_θ L(θ)
    Update θ ← θ - α * g
    (Optional) Adjust learning rate α
```

**주요 단계**

| 단계 | 설명 |
|------|------|
| 초기화 | 파라미터를 소규모 무작위값 등으로 설정 |
| 손실·기울기 계산 | 전체(배치) 데이터 혹은 일부(샘플/미니배치)를 이용해 \(L\) 와 \(\nabla L\) 구함 |
| 파라미터 업데이트 | 학습률 \(\alpha\) 와 기울기를 이용해 파라미터 이동 |
| 종료 판단 | 손실 감소 폭이 작아졌을 때, 혹은 사전에 정한 epoch 수에 도달했을 때 종료 |

---

## 변형·확장
### 4.1 확률적 경사하강법 (SGD)
- **정의** : 한 번의 업데이트에 **단일 샘플** \(x_i\) 로부터 기울기를 추정한다.  

\[
\theta^{(t+1)} = \theta^{(t)} - \alpha \, \nabla_{\theta} L_i(\theta^{(t)})
\]

- **특징** : 잡음이 섞인 추정값이지만, 빠른 초기 수렴과 큰 데이터셋에 적합.

### 4.2 미니배치 경사하강법
- **정의** : \(B\)개의 샘플을 한 번에 사용해 평균 기울기를 계산한다.  

\[
g^{(t)} = \frac{1}{|B|}\sum_{i\in B}\nabla_{\theta} L_i(\theta^{(t)})
\qquad
\theta^{(t+1)} = \theta^{(t)} - \alpha g^{(t)}
\]

- **장점** : GPU/CPU 병렬 처리에 최적화, 잡음 감소 + 빠른 업데이트.

### 4.3 모멘텀 (Momentum)
- **아이디어** : 이전 업데이트 방향을 누적해 관성(관성을 부여)한다.  

\[
v^{(t+1)} = \beta v^{(t)} + (1-\beta) g^{(t)}
\qquad
\theta^{(t+1)} = \theta^{(t)} - \alpha v^{(t+1)}
\]

- \(\beta\) (통상 0.9) 는 감쇠 계수.

### 4.4 적응 학습률 방법
| 알고리즘 | 핵심 아이디어 |
|----------|---------------|
| **AdaGrad** | 각 파라미터마다 누적된 제곱 기울기 \(\sum_{k=1}^{t} g_k^2\) 으로 학습률을 감소시킴 |
| **RMSprop** | 누적 제곱 기울기의 **지수 이동 평균**을 사용해 학습률을 안정화 |
| **Adam** (Adaptive Moment Estimation) | 모멘텀과 RMSprop을 결합; 1차/2차 모멘트 추정값을 **편향 보정** 후 사용 |

---

## 학습률 선택
- **고정 학습률** : 실험적으로 0.01~0.001 사이가 일반적.  
- **학습률 스케줄링**  
  - **Step Decay**: 일정 epoch마다 \(\alpha\) 를 감소.  
  - **Exponential Decay**: \(\alpha_t = \alpha_0 \cdot \gamma^{t}\) (\(0<\gamma<1\)).  
  - **Cosine Annealing**, **Cyclical LR** 등 최신 기법.  
- **학습률 탐색**: *Learning Rate Finder*(Leslie Smith) 로 최적 구간 탐색.

---

## 수렴 이론
- **볼츠만 평탄도 가정**: 손실이 **Lipschitz 연속**하고 **strongly convex**이면, 적절한 \(\alpha\) 로 **선형 수렴**이 보장된다.  
- **비볼록 함수**(예: 신경망)에서는 전역 수렴이 일반적으로 보장되지 않지만, 경험적으로 SGD + 모멘텀/Adam 은 좋은 로컬 최소점에 도달한다.  
- **수렴 조건 (단일 파라미터)**  
  \[
  0 < \alpha < \frac{2}{L_{\text{max}}}
  \]
  여기서 \(L_{\text{max}}\) 은 손실 함수의 **최대 이차 미분값**(리프시츠 상수)이다.

---

## 장점·단점
| 장점 | 단점 |
|------|------|
| 구현이 간단하고 직관적 | 학습률 민감성 → 튜닝 비용 |
| 대규모 데이터에 적용 가능 (SGD, 미니배치) | 비볼록 문제에서 로컬 최소점에 머무를 수 있음 |
| 다양한 변형(모멘텀, Adam 등)으로 성능 개선 가능 | 기울기 계산이 비용‑집약적 (특히 전체 배치) |
| 확장성이 높아 딥러닝 프레임워크에 기본 제공 | 학습률 스케줄링과 하이퍼파라미터 조정이 복잡 |

---

## 응용 사례
| 분야 | 구체적 적용 예시 |
|------|-------------------|
| **머신러닝** | 선형 회귀, 로지스틱 회귀, SVM(경사하강식) |
| **딥러닝** | CNN, RNN, Transformer 등 가중치 최적화 |
| **강화학습** | 정책 파라미터 업데이트 (REINFORCE) |
| **컴퓨터 비전** | 이미지 분할, 객체 검출 모델 학습 |
| **자연어 처리** | BERT, GPT 등 대형 언어 모델 사전학습 |
| **재무·경제** | 포트폴리오 최적화, 시계열 모델 파라미터 추정 |

---

## 구현 예시 (Python)

```python
import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000, batch_size=None):
    """
    선형 회귀 모델에 대한 기본 배치/미니배치 경사하강법 구현
    X : (n_samples, n_features) 입력 행렬
    y : (n_samples,) 타깃 벡터
    """
    n_samples, n_features = X.shape
    # 가중치와 bias 초기화
    w = np.zeros(n_features)
    b = 0.0

    # 배치 크기 지정 (None이면 전체 배치)
    if batch_size is None:
        batch_size = n_samples

    for epoch in range(epochs):
        # 데이터 셔플
        idx = np.random.permutation(n_samples)
        X_shuffled = X[idx]
        y_shuffled = y[idx]

        for start in range(0, n_samples, batch_size):
            end = start + batch_size
            X_batch = X_shuffled[start:end]
            y_batch = y_shuffled[start:end]

            # 예측 및 손실
            y_pred = X_batch.dot(w) + b
            error = y_pred - y_batch

            # 기울기 계산
            grad_w = (2 / len(X_batch)) * X_batch.T.dot(error)
            grad_b = (2 / len(X_batch)) * np.sum(error)

            # 파라미터 업데이트
            w -= lr * grad_w
            b -= lr * grad_b

        # (선택) 진행 상황 출력
        if (epoch + 1) % 100 == 0:
            loss = np.mean(error ** 2)
            print(f"Epoch {epoch+1:4d} | Loss: {loss:.6f}")

    return w, b

# ------------------------------------------------------------------
# 사용 예시
if __name__ == "__main__":
    # 간단한 2차원 데이터 생성
    np.random.seed(42)
    X = np.random.randn(500, 3)
    true_w = np.array([1.5, -2.0, 0.7])
    true_b = 0.3
    y = X @ true_w + true_b + 0.1 * np.random.randn(500)

    w_est, b_est = gradient_descent(X, y, lr=0.05, epochs=1000, batch_size=32)
    print("\nEstimated w:", w_est)
    print("Estimated b:", b_est)
```

*위 코드는 순수 NumPy 기반으로, 배치·미니배치·학습률 조절을 모두 지원한다. 실제 프로젝트에서는 PyTorch, TensorFlow 등 자동 미분 프레임워크를 활용해 보다 효율적인 구현을 한다.*

---

## 참고 문헌
1. **Bishop, C. M.** *Pattern Recognition and Machine Learning*. Springer, 2006. – 손실 함수와 최적화 기초.  
2. **Ruder, S.** *An Overview of Gradient Descent Optimization Algorithms*. arXiv:1609.04747, 2016. – 다양한 변형 정리.  
3. **Kingma, D. P., & Ba, J.** *Adam: A Method for Stochastic Optimization*. ICLR, 2015.  
4. **LeCun, Y., Bottou, L., Orr, G. B., & Müller, K.-R.** *Efficient BackProp*. Neural Networks: Tricks of the Trade, 1998. – 학습률 스케줄링 및 모멘텀.  
5. **Goodfellow, I., Bengio, Y., & Courville, A.** *Deep Learning*. MIT Press, 2016. – 딥러닝에서의 SGD와 그 변형.  

---

## 외부 링크
- [Wikipedia – Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)  
- [Stanford CS231n – Lecture 8: Optimization](http://cs231n.stanford.edu/)  
- [Deep Learning Book – Chapter 8 (Optimization for Training Deep Models)](https://www.deeplearningbook.org/)  
- [PyTorch – torch.optim](https://pytorch.org/docs/stable/optim.html)  

---  

*이 문서는 2025년 현재까지의 경사하강법 이론·실무에 대한 요약이며, 최신 연구 동향(예: Second‑order 메서드, 샤프니스 기반 학습률 조정 등)은 별도 리뷰 문헌을 참고한다.*