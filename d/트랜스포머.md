# 트랜스포머

트랜스포머(Transformer)는 2017년 구글 브레인(구글 연구소)에서 발표한 **“Attention Is All You Need”** 논문을 통해 처음 소개된 딥러닝 모델이다. 순환 신경망(RNN)이나 합성곱 신경망(CNN) 대신 **자기‑주의(self‑attention)** 메커니즘만을 사용해 입력 시퀀스를 병렬적으로 처리할 수 있다는 핵심 아이디어로, 자연어 처리(NLP) 분야에 혁신적인 변화를 일으켰으며 이후 컴퓨터 비전, 음성 인식 등 다양한 도메인으로 확장되었다.

---

## 개요

트랜스포머는 **인코더-디코더 구조**를 기본으로 하며, 인코더와 디코더는 모두 여러 개의 동일한 레이어를 쌓아 만든다. 각 레이어는 자기‑주의와 피드포워드 완전 연결층(FFN)으로 구성된다. 인코더는 입력 토큰 간의 전역적인 관계를 학습하고, 디코더는 인코더의 출력과 이전에 생성된 토큰을 이용해 다음 토큰을 예측한다.

핵심 특징은 다음과 같다.

- **완전 병렬 처리**: 순차적인 계산이 필요 없는 자기‑주의 덕분에 GPU/TPU와 같은 하드웨어에서 효율적인 학습이 가능하다.  
- **장기 의존성 학습**: 모든 토큰이 서로 직접 연결되므로 매우 긴 시퀀스에서도 관계를 학습할 수 있다.  
- **스케일링 용이성**: 레이어 수, 머리 수, 차원 등을 늘려 모델 크기를 손쉽게 확장할 수 있다.

---

## 역사

| 연도 | 주요 사건 |
|------|-----------|
| 2017 | Vaswani **et al.**가 *Attention Is All You Need* 논문에서 트랜스포머 제안 |
| 2018 | Google의 **BERT**(Bidirectional Encoder Representations from Transformers) 발표 – 사전학습(pre‑training)과 미세조정(fine‑tuning) 패러다임 도입 |
| 2019 | OpenAI의 **GPT-2**, **GPT-3** 등 대규모 생성형 언어 모델 등장 |
| 2020 | **Transformer-XL**, **Reformer**, **Longformer** 등 장기 시퀀스 처리를 위한 변형 모델 발표 |
| 2021~2023 | **Vision Transformer (ViT)**, **DETR**, **Perceiver** 등 비전 및 멀티모달 분야로 확장 |
| 2024 | **GPT‑4**, **LLaMA**, **Claude** 등 초대규모 모델이 상용화 및 오픈소스로 공개 |

---

## 구조와 핵심 아이디어

### 1. 자기‑주의(Self‑Attention)

입력 시퀀스 \(X = (x_1, \dots, x_n)\)에 대해 각 토큰을 **쿼리(Q)**, **키(K)**, **값(V)** 로 변환한다.

\[
\begin{aligned}
Q &= XW_Q, \\
K &= XW_K, \\
V &= XW_V,
\end{aligned}
\]

여기서 \(W_Q, W_K, W_V\)는 학습 가능한 행렬이다.  
각 토큰 간 유사도는 **스케일된 점곱(dot‑product)**을 사용해 계산한다.

\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]

\(d_k\)는 키 차원 수로, 스케일링을 통해 수치적 안정성을 확보한다.

### 2. 다중 헤드(Multi‑Head) 어텐션

단일 어텐션 헤드만으로는 제한된 표현력을 갖는다. 따라서 \(h\)개의 어텐션을 **병렬**로 수행하고, 결과를 연결(concatenate)한 뒤 선형 변환한다.

\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1,\dots,\text{head}_h)W_O
\]

각 헤드는 서로 다른 서브스페이스에서 정보를 추출한다.

### 3. 포지션 인코딩(Position Encoding)

자기‑주의는 순서를 고려하지 않으므로, 입력 토큰에 순서 정보를 주입하기 위해 **포지션 인코딩**을 더한다. 원 논문에서는 사인·코사인 함수를 이용한 고정 인코딩을 사용했으며, 이후 학습 가능한 위치 임베딩도 널리 쓰인다.

\[
PE_{(pos,2i)} = \sin\!\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right),\quad
PE_{(pos,2i+1)} = \cos\!\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]

### 4. 레이어 정규화와 잔차 연결

각 서브 레이어(자기‑주의, 피드포워드) 뒤에는 **잔차 연결(residual connection)**과 **레이어 정규화(LayerNorm)**을 적용해 학습 안정성을 높인다.

\[
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
\]

### 5. 포지션-와이즈 피드포워드 네트워크

각 토큰에 독립적으로 적용되는 두 개의 선형 변환 사이에 비선형 활성화 함수(ReLU 혹은 GeLU)를 삽입한다.

\[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\]

---

## 변형 및 파생 모델

트랜스포머의 기본 구조를 바탕으로 다양한 목적에 맞게 수정·확장된 모델이 많이 등장한다.

| 모델 | 주요 특징 | 적용 분야 |
|------|-----------|-----------|
| **BERT** | 양방향 인코더, 마스크드 언어 모델링(MLM) | 텍스트 이해, 질의응답, 문장 분류 |
| **GPT 시리즈** | 단방향 디코더, 자동 회귀 언어 모델링 | 텍스트 생성, 대화 시스템 |
| **Transformer‑XL** | 재사용 가능한 숨은 상태, 상대 위치 인코딩 | 긴 문서 처리, 언어 모델링 |
| **Reformer** | 로컬 민감 해시(LSH) 어텐션, 메모리 효율적인 역전파 | 초대규모 모델 학습 |
| **Longformer** | 슬라이딩 윈도우 + 전역 어텐션 | 긴 텍스트(논문, 책) |
| **Vision Transformer (ViT)** | 이미지 패치를 토큰화해 트랜스포머 적용 | 이미지 분류, 비전 작업 |
| **DETR** | 객체 검출을 위한 엔드‑투‑엔드 트랜스포머 | 객체 검출 |
| **Perceiver** | 입력 차원을 압축하는 교차 어텐션 | 멀티모달(이미지+텍스트) |
| **Mistral, LLaMA, Falcon** 등 | 효율적인 파라미터 설계와 공개 코드 | 오픈소스 대형 언어 모델 |

---

## 학습 방법

### 1. 사전학습(Pre‑training)

- **마스크드 언어 모델링(MLM)**: 입력 토큰 중 일부를 마스크([MASK])하고 원래 토큰을 예측하도록 학습. (예: BERT)  
- **자동 회귀 언어 모델링(ARLM)**: 시퀀스의 앞부분을 보고 다음 토큰을 예측. (예: GPT)  
- **시퀀스‑투‑시퀀스(S2S)**: 입력 → 출력 형태의 번역 등에서 사용. (예: T5)

대규모 텍스트 코퍼스(예: Wikipedia, Common Crawl)를 이용해 수십억~수조 개의 파라미터를 학습한다.

### 2. 미세조정(Fine‑tuning)

사전학습된 모델을 특정 다운스트림 태스크에 맞게 추가 레이어를 붙이거나 전체 파라미터를 재학습한다. 일반적으로 **학습률 스케줄링**, **드롭아웃**, **레이어 별 학습률** 등을 활용한다.

### 3. 효율화 기법

- **Mixed Precision Training** (FP16) → 메모리 절감 + 속도 향상  
- **Gradient Checkpointing** → 중간 활성값을 재계산해 메모리 사용량 감소  
- **Adapter Modules** → 소수의 파라미터만 학습해 파인튜닝 비용 최소화  
- **LoRA (Low‑Rank Adaptation)** → 저차원 업데이트로 효율적인 파인튜닝 제공  

---

## 적용 분야

### 자연어 처리

- **텍스트 분류** (감성 분석, 스팸 탐지)
- **질의응답** (SQuAD, MRC)
- **기계 번역** (Transformer‑based NMT)
- **요약** (Extractive / Abstractive)
- **대화 시스템** (ChatGPT, 음성 비서)

### 컴퓨터 비전

- **이미지 분류** (ViT, DeiT)
- **객체 검출** (DETR)
- **이미지 캡셔닝** (ViT + Text Transformer)
- **비디오 이해** (TimeSformer, Video Swin Transformer)

### 음성 및 오디오

- **음성 인식** (Conformer)
- **음성 합성** (FastSpeech)
- **음악 생성** (Music Transformer)

### 멀티모달 및 기타

- **텍스트‑이미지 생성** (DALL·E, Stable Diffusion)
- **로봇 제어** (Transformer‑based policy networks)
- **생물 정보학** (단백질 구조 예측, AlphaFold)

---

## 한계와 최신 연구 동향

- **연산·메모리 비용**: 모델 규모가 커질수록 GPU/TPU 자원이 급증한다. 이를 줄이기 위해 **Sparse Attention**, **Mixture‑of‑Experts**, **Quantization** 등 연구가 활발히 진행 중이다.
- **데이터 편향 및 윤리 문제**: 대규모 코퍼스에 내재된 사회적 편향이 모델 출력에 반영될 위험이 있다. **데이터 정제**, **공정성 평가**, **조정(Alignment)** 기술이 중요해지고 있다.
- **합성곱·순환과의 하이브리드**: 국소적 패턴 인식에 강한 CNN·RNN과 전역적 관계 학습에 강한 트랜스포머를 결합한 구조가 제안되고 있다.
- **대규모 모델의 환경 비용**: 에너지 효율과 탄소 발자국을 최소화하기 위한 **녹색 AI** 연구가 강조된다.
- **설명 가능성(Explainability)**: 어텐션 맵을 이용한 해석 기법, **프롬프트 엔지니어링** 등 모델 내부를 이해하려는 시도가 지속된다.

---

## 참고 문헌

1. Vaswani, A. *et al.* (2017). **Attention Is All You Need**. *Advances in Neural Information Processing Systems* 30.  
2. Devlin, J. *et al.* (2018). **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**. *arXiv preprint arXiv:1810.04805*.  
3. Radford, A. *et al.* (2019). **Language Models are Unsupervised Multitask Learners**. *OpenAI Blog*.  
4. Dai, Z. *et al.* (2019). **Transformer‑XL: Attentive Language Models Beyond a Fixed-Length Context**. *ACL*.  
5. Dosovitskiy, A. *et al.* (2020). **An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale**. *ICLR*.  
6. Carion, N. *et al.* (2020). **End‑to‑End Object Detection with Transformers**. *ECCV*.  
7. Brown, T. *et al.* (2020). **Language Models are Few‑Shot Learners**. *NeurIPS*.  
8. Liu, Y. *et al.* (2021). **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**. *ICCV*.  
9. Chowdhery, A. *et al.* (2022). **PaLM: Scaling Language Modeling with Pathways**. *arXiv preprint arXiv:2204.02311*.  
10. Wang, A. *et al.* (2023). **Vision Transformer in Computer Vision: A Survey**. *IEEE Transactions on Pattern Analysis and Machine Intelligence*.  

*(※ 최신 논문 및 발표는 2024년 이후에도 지속적으로 추가되고 있다.)*  

---  

*이 문서는 2025년 8월 기준 최신 연구 동향을 반영하여 작성되었습니다.*