# 강화 학습 (Reinforcement Learning)

*위키 문서 형식*  
*작성일: 2025‑08‑17*  
*작성자: ChatGPT*  

---  

## 목차
1. [개요](#개요)  
2. [역사](#역사)  
3. [핵심 개념](#핵심-개념)  
    - 3.1 [에이전트와 환경](#에이전트와-환경)  
    - 3.2 [상태, 행동, 보상](#상태-행동-보상)  
    - 3.3 [정책 (Policy)](#정책-policy)  
    - 3.4 [가치 함수 (Value Function)](#가치-함수-value-function)  
    - 3.5 [모델 (Model)](#모델)  
4. [주요 알고리즘](#주요-알고리즘)  
    - 4.1 [동적 프로그래밍](#동적-프로그래밍)  
    - 4.2 [몬테카를로 방법](#몬테카를로-방법)  
    - 4.3 [시간 차이 학습 (TD Learning)](#시간-차이-학습-td-learning)  
    - 4.4 [Q‑학습](#q-학습)  
    - 4.5 [SARSA](#sarsa)  
    - 4.6 [정책 경사 (Policy Gradient) 방법]  
    - 4.7 [Actor‑Critic 구조]  
    - 4.8 [심층 강화 학습 (Deep RL)]  
5. [응용 분야](#응용-분야)  
6. [연구 동향 및 주요 이슈](#연구-동향-및-주요-이슈)  
7. [참고 문헌](#참고-문헌)  
8. [외부 링크](#외부-링크)  

---  

## 개요
강화 학습(Reinforcement Learning, RL)은 **에이전트가 환경과 상호작용하면서 보상(reward)을 최대화하도록 학습**하는 기계 학습 패러다임이다. 지도 학습이 입력–출력 쌍을 제공하는 반면, 강화 학습은 **시행착오(trial‑and‑error)** 를 통해 최적의 행동 전략(policy)을 스스로 찾는다.  

- **에이전트(Agent)** : 의사결정을 수행하는 주체.  
- **환경(Environment)** : 에이전트가 관찰하고 행동을 취하는 대상.  
- **보상(Reward)** : 현재 행동이 얼마나 좋은지를 나타내는 스칼라 값.  

강화 학습은 로봇 제어, 게임 인공지능, 자율 주행, 금융 트레이딩, 네트워크 최적화 등 **연속적·동적**인 문제에 널리 활용된다.  

---  

## 역사
| 연도 | 주요 사건 / 논문 | 비고 |
|------|------------------|------|
| 1950‑1960 | **동적 프로그래밍** (Bellman) | 최적 제어 이론의 기초, Bellman equation 제시 |
| 1983 | **Temporal‑Difference (TD) Learning** (Sutton) | TD(0) 소개, 강화 학습의 핵심 아이디어 |
| 1989 | **Q‑Learning** (Watkins) | 모델 프리(model‑free) 오프‑폴리시 학습 |
| 1992 | **SARSA** (Rummery & Niranjan) | 온‑폴리시(on‑policy) TD 학습 |
| 1998 | **Policy Gradient** (Williams) | REINFORCE 알고리즘 발표 |
| 2013 | **Deep Q‑Network (DQN)** (Mnih et al., Nature) | 심층 신경망과 Q‑Learning 결합, Atari 게임 성공 |
| 2015‑2016 | **A3C, DDPG, TRPO, PPO** 등 | 병렬 학습·연속 제어·안정적 정책 최적화 등 다양한 심층 RL 알고리즘 등장 |
| 2020‑2024 | **멀티모달 RL, 메타‑RL, 오프‑라인 RL, LLM‑기반 RL** | 대규모 언어 모델·시뮬레이션 기반 학습 확대 |

---  

## 핵심 개념

### 에이전트와 환경
- **에이전트**는 현재 **상태** \(s_t\) 를 관찰하고, **행동** \(a_t\) 를 선택한다.  
- **환경**은 에이전트의 행동을 받아 **다음 상태** \(s_{t+1}\) 와 **보상** \(r_{t+1}\) 을 반환한다.  

### 상태, 행동, 보상
- **상태 \(S\)** : 환경을 기술하는 변수들의 집합. (예: 이미지 픽셀, 로봇 관절 각도)  
- **행동 \(A\)** : 에이전트가 취할 수 있는 선택지. 이산형(분류) 혹은 연속형(실수)일 수 있다.  
- **보상 \(R\)** : 단일 시점에서 받은 즉시값. 누적 보상(리턴) \(G_t = \sum_{k=0}^{\infty}\gamma^k r_{t+k+1}\) 로 정의한다.  

### 정책 (Policy)
- **정책 \(\pi\)** : 상태를 행동에 매핑하는 함수.  
    - **Deterministic** : \(\pi(s) = a\)  
    - **Stochastic** : \(\pi(a|s) = P(a|s)\)  

### 가치 함수 (Value Function)
- **상태 가치 \(V^{\pi}(s)\)** : 현재 정책 \(\pi\) 하에서 상태 \(s\) 에서 시작해 얻을 기대 누적 보상.  
  \[
  V^{\pi}(s)=\mathbb{E}_{\pi}\left[ G_t \mid s_t=s \right]
  \]  

- **행동 가치 \(Q^{\pi}(s,a)\)** : 상태‑행동 쌍에 대한 기대 누적 보상.  
  \[
  Q^{\pi}(s,a)=\mathbb{E}_{\pi}\left[ G_t \mid s_t=s, a_t=a \right]
  \]  

- **벨만 방정식** (Bellman Equation) 은 가치 함수 간 관계를 정의한다.  

### 모델 (Model)
- **Model‑Based RL** : 전이 확률 \(P(s'|s,a)\) 와 보상 함수 \(R(s,a,s')\) 를 학습하거나 이용한다.  
- **Model‑Free RL** : 직접적인 전이/보상 모델 없이 가치 함수 혹은 정책만 학습한다.  

---  

## 주요 알고리즘

### 동적 프로그래밍
- **Policy Iteration**, **Value Iteration** 등.  
- 완전한 전이 모델이 필요하고, 작은 MDP에 적용 가능.  

### 몬테카를로 방법
- 에피소드 전체를 시뮬레이션 후 평균을 사용해 가치 추정.  
- 편향은 없지만 높은 분산을 가짐.  

### 시간 차이 학습 (TD Learning)
- **TD(0)** : 한 단계 예측 오류(temporal‑difference error) 를 이용해 즉시 업데이트.  
- **TD(λ)** : Eligibility trace 를 도입해 다중 단계 정보를 결합.  

### Q‑학습
- **오프‑폴리시** 모델‑프리 알고리즘.  
- 업데이트:  
  \[
  Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha\big[r_{t+1} + \gamma \max_{a'}Q(s_{t+1},a') - Q(s_t,a_t)\big]
  \]  

### SARSA
- **온‑폴리시** TD 제어.  
- 업데이트:  
  \[
  Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha\big[r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\big]
  \]  

### 정책 경사 (Policy Gradient) 방법
- 목표: \(\max_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}}[G]\)  
- **REINFORCE** : 샘플링된 경로에 대한 Monte‑Carlo 추정, 기본 식:  
  \[
  \nabla_{\theta} J(\theta) \approx \sum_{t} \nabla_{\theta}\log \pi_{\theta}(a_t|s_t) \, G_t
  \]  

### Actor‑Critic 구조
- **Actor** : 정책 \(\pi_{\theta}\) 를 직접 파라미터화.  
- **Critic** : 가치 함수 \(V_{w}(s)\) 로 행동의 질을 평가하고, TD 오차를 활용해 Actor 를 업데이트.  

### 심층 강화 학습 (Deep RL)
| 알고리즘 | 주요 특징 | 대표 논문 |
|----------|-----------|-----------|
| DQN (Deep Q‑Network) | 경험 재플레이, 타깃 네트워크 | Mnih et al., 2015 |
| Double DQN | 과대평가 감소 | van Hasselt et al., 2016 |
| Dueling DQN | 상태‑가치와 어드밴트지 분리 | Wang et al., 2016 |
| A3C (Asynchronous Advantage Actor‑Critic) | 다중 스레드 비동기 학습 | Mnih et al., 2016 |
| PPO (Proximal Policy Optimization) | 클리핑 손실로 안정성 향상 | Schulman et al., 2017 |
| SAC (Soft Actor‑Critic) | 엔트로피 정규화로 탐사·수렴 균형 | Haarnoja et al., 2018 |
| TD3 (Twin Delayed DDPG) | 연속 제어용 DDPG 개선 | Fujimoto et al., 2018 |
| MuZero | 모델‑프리와 모델‑베이스 혼합 (트랜지션 모델 학습) | Schrittwieser et al., 2020 |

---  

## 응용 분야
| 분야 | 구체적인 적용 사례 |
|------|-------------------|
| **게임** | 알파고·알파스타, OpenAI Five, DeepMind AlphaStar, Atari DQN |
| **로보틱스** | 로봇 팔의 궤적 최적화, 보행 로봇 제어, 비전‑기반 조작 |
| **자율 주행** | 차선 유지, 교차로 통과, 복합 교통 시뮬레이션 |
| **자연어 처리** | 대화형 에이전트, 텍스트 요약(Reward‑based), LLM‑fine‑tuning via RLHF |
| **금융·거래** | 포트폴리오 최적화, 고빈도 트레이딩 전략 |
| **네트워크·통신** | 라우팅, 자원 할당, 매체 접근 제어 |
| **헬스케어** | 치료 계획 최적화, 약물 설계, 임상 시험 설계 |
| **산업 자동화** | 공정 제어, 물류 로봇, 스마트 팩토리 스케줄링 |
| **에너지** | 전력 그리드 관리, 배터리 충전 스케줄링 |
| **멀티에이전트** | 협동/경쟁 시뮬레이션, UAV 군집, 게임 AI 군대 |

---  

## 연구 동향 및 주요 이슈
1. **오프라인(배치) 강화 학습** – 기존 데이터를 재활용해 정책을 학습, 안전성·데이터 효율성 강조.  
2. **사전학습된 정책과 메타‑RL** – 여러 태스크에 일반화 가능한 초기 정책 학습.  
3. **안전·공정성·해석 가능성** – 보상 설계의 편향, 정책 검증, 인간‑인증 보조 시스템.  
4. **대규모 언어 모델과 RLHF (Reinforcement Learning from Human Feedback)** – ChatGPT, Claude 등 LLM 의 정교한 행동 제어.  
5. **멀티모달 RL** – 비전, 언어, 촉각 등 다양한 감각 정보를 동시에 활용.  
6. **계산 효율성** – 샘플 효율성 향상을 위한 모델‑기반/모델‑프리 하이브리드, 비동기 학습, 분산 프레임워크.  

---  

## 참고 문헌
1. Sutton, R. S., & Barto, A. G. (2018). **Reinforcement Learning: An Introduction** (2nd ed.). MIT Press.  
2. Watkins, C. J. C. H., & Dayan, P. (1992). *Q‑learning*. Machine Learning, 8, 279‑292.  
3. Mnih, V. et al. (2015). *Human‑level control through deep reinforcement learning*. Nature, 518, 529‑533.  
4. Schulman, J. et al. (2017). *Proximal Policy Optimization Algorithms*. arXiv:1707.06347.  
5. Haarnoja, T. et al. (2018). *Soft Actor‑Critic: Off‑Policy Maximum Entropy Deep RL*. ICML.  
6. Silver, D. et al. (2016). *Mastering the game of Go with deep neural networks and tree search*. Nature, 529, 484‑489.  
7. OpenAI (2023). *ChatGPT: Training and RL from Human Feedback*. arXiv:2303.02155.  

---  

## 외부 링크
- [DeepMind Publications](https://deepmind.com/research/publications)  
- [OpenAI Blog – Reinforcement Learning](https://openai.com/blog/reinforcement-learning)  
- [RL™ OpenAI Gym](https://gym.openai.com) – 다양한 강화 학습 환경 제공  
- [Berkeley CS 285: Deep Reinforcement Learning](http://deepreinforcementlearning.cs.berkeley.edu) – 강의 노트 및 코스 자료  

---  

*이 문서는 2025‑08‑17 기준 최신 연구와 일반적인 교과서 내용을 통합하여 작성되었습니다. 지속적인 업데이트가 필요합니다.*