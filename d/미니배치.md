# 미니배치 (Mini‑batch)

미니배치는 딥러닝·머신러닝 모델을 학습시킬 때 **전체 데이터셋을 여러 개의 작은 부분(배치)으로 나누어** 순차적으로 파라미터를 업데이트하는 방법이다. 전체 데이터를 한 번에 사용해 파라미터를 갱신하는 **배치 학습(batch learning)** 과, 하나의 샘플만 사용해 갱신하는 **순차(온라인) 학습(stochastic learning)** 의 중간 형태로, 메모리 효율성·연산 효율성·학습 안정성 측면에서 널리 활용된다.

---

## 정의

- **미니배치**: 전체 학습 데이터를 `N`개의 샘플로 구성된 **배치**(`batch`) 단위로 구분하여, 각 배치에 대해 **경사 하강법(Gradient Descent)** 혹은 그 변형(예: Adam, RMSProp 등)으로 파라미터를 업데이트하는 방식이다.  
- **배치 크기(batch size)**: 한 번에 모델에 입력되는 샘플 수. 일반적으로 2ⁿ(예: 32, 64, 128 …) 형태를 많이 사용한다.

> “배치 크기는 메모리 한계와 GPU/TPU의 병렬 연산 효율 사이의 트레이드오프를 반영한다.” ― *Krizhevsky et al., 2012*[^1]

---

## 배경 및 역사

| 연도 | 주요 논문 / 사건 | 핵심 내용 |
|------|------------------|-----------|
| 1998 | **LeNet‑5** (LeCun) | 초기 컨볼루션 신경망에서 GPU가 없었음에도 작은 미니배치를 활용해 효율적인 학습을 시도 |
| 2012 | **AlexNet** (Krizhevsky) | GPU 기반 대규모 이미지 분류에서 배치 크기 128을 사용, 미니배치가 현대 딥러닝의 표준이 되도록 촉진 |
| 2015–2020 | **Batch Normalization** (Ioffe, Szegedy) | 미니배치 통계(평균, 분산)를 이용해 정규화, 학습 가속화와 일반화 성능 향상 |
| 2022‑ | **Large‑Batch Training** 연구 | 배치 크기를 수천~수만까지 확대하면서도 학습 안정성을 유지하기 위한 스케줄링·옵티마이저 개선 연구 |

---

## 작동 원리

### 학습 루프

1. **데이터 셔플** – 에포크 시작 시 전체 데이터를 무작위 순서로 재배열한다.  
2. **배치 추출** – 배치 크기 `B`에 따라 연속된 `B`개의 샘플을 선택한다.  
3. **전방 전달(Forward Pass)** – 선택된 배치에 대해 모델 출력을 계산한다.  
4. **손실 계산** – 배치 전체 손실을 평균(또는 합)하여 하나의 스칼라 값으로 만든다.  
5. **역전파(Backward Pass)** – 손실에 대한 그래디언트를 계산하고, 옵티마이저를 통해 파라미터를 업데이트한다.  
6. **반복** – 전체 데이터가 소진될 때까지 2–5 과정을 반복한다(한 에포크).  

```python
# PyTorch 예시
for epoch in range(num_epochs):
    # 1️⃣ 셔플
    permutation = torch.randperm(train_dataset.size(0))
    for i in range(0, train_dataset.size(0), batch_size):
        # 2️⃣ 배치 추출
        indices = permutation[i:i+batch_size]
        batch_x, batch_y = train_dataset[indices]
        
        # 3️⃣ 전방 전달
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)          # 4️⃣ 손실 평균
        
        # 5️⃣ 역전파 및 파라미터 업데이트
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 배치 형성 전략

| 전략 | 특징 | 장점 | 단점 |
|------|------|------|------|
| **고정 크기** | 매 배치마다 동일한 `B` | 구현이 간단, GPU 메모리 예측 가능 | 데이터가 `B`의 배수가 아니면 마지막 배치가 작아질 수 있음 |
| **동적 크기** | 배치 크기를 상황에 따라 조정 (예: Gradient Accumulation) | 메모리 제한을 넘어서는 대규모 모델 학습 가능 | 구현 복잡성 증가 |
| **Stratified Mini‑batch** | 클래스 비율을 유지하도록 샘플링 | 클래스 불균형 문제 완화 | 데이터 전처리 비용 ↑ |

---

## 수학적 표현

전체 데이터를 `X = {x₁,…,x_N}`라 하고, 파라미터를 `θ`라 하자. 배치 `B_k`는 `B`개의 인덱스를 포함한다. 미니배치를 사용한 경사 하강법 업데이트는 다음과 같다.

$
\theta^{(t+1)} = \theta^{(t)} - \eta \, \frac{1}{|B_k|}\sum_{i\in B_k}\nabla_{\theta}\, \mathcal{L}\bigl(f_\theta(x_i), y_i\bigr)
$

- `η` : 학습률 (learning rate)  
- `𝓛` : 손실 함수 (예: 교차 엔트로피, MSE)  

배치 크기가 `|B_k| = N`이면 **배치 경사 하강법**, `|B_k| = 1`이면 **확률적 경사 하강법**이 된다.

---

## 장점

- **메모리 효율** – 전체 데이터를 한 번에 로드할 필요가 없어 GPU/TPU 메모리 사용량을 제어한다.  
- **연산 병렬화** – 동일한 연산을 `B`번 묶어 한 번에 수행함으로써 SIMD(Single Instruction, Multiple Data) 가속을 극대화한다.  
- **노이즈와 일반화** – 배치마다 그래디언트가 약간씩 달라져 **노이즈 주입** 효과가 생겨 지역 최소점에 머무르지 않고 더 좋은 일반화 성능을 얻는다.  
- **학습률 스케줄링** – 배치 크기에 따라 초기 학습률을 조절하거나 `Linear Scaling Rule`(배치 크기 배율에 따라 학습률을 선형 확대) 등을 적용하기 용이하다.  

---

## 단점 및 주의점

- **배치 크기 선택 난이도** – 너무 작으면 학습이 불안정하고, 너무 크면 일반화가 저하될 수 있다.  
- **마지막 배치 문제** – 전체 데이터가 배치 크기의 배수가 아닐 경우, 마지막 배치가 작은 샘플 수를 갖게 되어 그래디언트 추정이 편향될 수 있다.  
- **동기화 오버헤드** – 다중 GPU/TPU 환경에서 배치 단위로 파라미터 동기화가 필요해 통신 비용이 증가한다.  
- **학습 속도 감소** – 배치가 작을수록 GPU 활용률이 떨어져 **연산 효율**이 감소할 수 있다.  

---

## 배치 크기 선택 가이드라인

| 배치 규모 | 권장 상황 | 일반적인 학습률(예시) | 비고 |
|-----------|-----------|----------------------|------|
| **≤ 32** | 메모리 제한이 심하거나, 고정된 작은 데이터셋 | `1e‑4` ~ `5e‑4` (낮은 학습률) | 높은 그래디언트 노이즈 → 강한 정규화 필요 |
| **64 ~ 256** | 대부분의 딥러닝 작업 (이미지, 텍스트) | `1e‑3` ~ `5e‑3` | 균형 잡힌 효율·일반화 |
| **≥ 512** | 대규모 이미지/비디오, 대형 언어 모델 (LLM) | `1e‑2` 이상 (Linear Scaling 적용) | **Gradient Accumulation** 필요 가능 |
| **수천 이상** | 초대형 모델(예: GPT‑3) | `> 1e‑2` (특수 스케줄링) | **LARS / LAMB** 옵티마이저 사용 권장 |

> **Tip**: 배치 크기가 커질수록 `learning_rate × batch_size` 곱을 일정하게 유지하는 **Linear Scaling Rule**이 효과적이다[^2].

---

## GPU/TPU와의 연관성

- **메모리 대역폭**: 배치를 크게 하면 메모리 읽기/쓰기 횟수가 감소해 대역폭 효율이 상승한다.  
- **연산 집약도**: 현대 GPU는 **CUDA Core**·**Tensor Core**를 활용해 행렬 연산을 크게 가속화한다. 배치 크기가 충분히 크면 이들 코어가 거의 풀 가동 상태가 된다.  
- **Mixed‑Precision**: 배치 크기가 클수록 `float16`(FP16) 혹은 `bfloat16`을 사용해도 안정적인 학습이 가능해 메모리 사용량을 절반 이하로 줄일 수 있다.  
- **멀티‑디바이스**: `DistributedDataParallel(DDP)`·`tf.distribute.Strategy`와 같은 프레임워크는 **배치 레벨**에서 데이터 병렬을 수행한다. 배치 크기가 작을 경우 **All‑Reduce** 통신 빈도가 상승해 오히려 성능 저하가 발생한다.

---

## 동적 미니배치와 정적 미니배치

| 구분 | 설명 | 사용 예시 |
|------|------|-----------|
| **정적 미니배치** | 학습 전체 동안 배치 크기가 고정 | 대부분의 표준 이미지·텍스트 분류 파이프라인 |
| **동적 미니배치** | 배치 크기를 에포크, 학습 단계, 메모리 사용량 등에 따라 변동 | *Gradient Accumulation* (큰 가상 배치를 구현), *Curriculum Learning* (초반 작은 배치 → 후반 큰 배치) |
| **가변 길이 시퀀스 배치** | NLP, 음성 처리 등에서 각 시퀀스 길이에 따라 패딩을 최소화하기 위해 배치 내 시퀀스 길이를 맞추는 전략 | `torch.nn.utils.rnn.pad_sequence`, `tf.keras.preprocessing.sequence.pad_sequences` |

---

## 주요 응용 사례

- **이미지 분류·객체 탐지** – AlexNet, ResNet, EfficientNet 등 대부분의 CNN은 32~256 배치를 사용한다.  
- **자연어 처리** – BERT, GPT 시리즈는 128~1024 배치를 적용하고, 대규모 사전학습 단계에서는 `gradient accumulation`을 통해 가상 배치 크기를 8 k 이상으로 확장한다.  
- **강화학습** – Experience Replay에 저장된 트랜지션을 미니배치로 샘플링해 DQN, DDPG 등에서 효율적인 Q‑함수 업데이트를 수행한다.  
- **시계열·시계열 예측** – LSTM/GRU 기반 모델에서 가변 길이 배치를 통해 메모리 사용을 최적화한다.  
- **그래프 신경망(GNN)** – 배치 크기 대신 **노드/서브그래프** 수를 조절해 메모리 한계를 관리한다.

---

## 실제 코드 예시

### PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 가상의 데이터(예: 10,000개, 784 차원)
X = torch.randn(10000, 784)
y = torch.randint(0, 10, (10000,))

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=3e-4)

for epoch in range(10):
    for batch_x, batch_y in loader:
        logits = model(batch_x)
        loss = criterion(logits, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1} ‑ loss: {loss.item():.4f}")
```

### TensorFlow (tf.keras)

```python
import tensorflow as tf

# 가상의 MNIST 형태 데이터
X = tf.random.normal([10000, 784])
y = tf.random.uniform([10000], maxval=10, dtype=tf.int32)

dataset = tf.data.Dataset.from_tensor_slices((X, y))
dataset = dataset.shuffle(10000).batch(128).prefetch(tf.data.AUTOTUNE)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(10)
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

model.fit(dataset, epochs=12)
```

**핵심 포인트**

- `batch_size` 파라미터가 바로 미니배치 크기다.  
- `shuffle=True`(PyTorch) / `shuffle()`(TensorFlow) 로 **데이터 셔플**을 수행해 배치 간 상관관계를 최소화한다.  
- `prefetch`와 `num_workers`를 적절히 활용하면 **I/O 병목**을 크게 완화할 수 있다.

---

## 관련 개념

| 용어 | 설명 |
|------|------|
| **Batch Normalization** | 배치 차원에서 평균·분산을 이용해 내부 공변량 이동(Internal Covariate Shift)을 줄이고 학습을 가속화한다. |
| **Gradient Accumulation** | 메모리 제한으로 큰 배치를 직접 형성하기 어려울 때, 작은 배치를 여러 번 수행하고 그래디언트를 누적해 가상 큰 배치를 구현한다. |
| **Learning Rate Warm‑up** | 초기 몇 에포크 동안 학습률을 점진적으로 증가시켜 대규모 배치 학습에서 불안정성을 완화한다. |
| **Data Parallelism** | 여러 디바이스에 동일 모델을 복제하고 각각 다른 배치를 처리해 파라미터를 동기화한다. |
| **Model Parallelism** | 모델 자체를 여러 디바이스에 분할해 큰 모델을 학습한다. 배치 크기와는 별개지만 큰 배치와 결합될 때 메모리 관리가 핵심이다. |

---

## 오해와 흔한 질문

- **Q1. “배치를 크게 하면 언제나 학습이 빨라진다?”**  
  - **A:** 연산량은 동일하지만, 배치가 너무 크면 **학습률 스케줄링**이나 **옵티마이저**가 적절히 조정되지 않아 수렴이 지연될 수 있다. 또한 GPU 메모리 초과는 학습 중단을 초래한다.

- **Q2. “미니배치가 없으면 전체 데이터로만 학습한다는 의미인가?”**  
  - **A:** 전체 데이터(전체 배치) 학습은 **배치 경사 하강법**이며, 메모리 요구가 커서 대규모 데이터셋에서는 실용적이지 않다.

- **Q3. “배치 크기를 1로 하면 순수 확률적 경사 하강법과 같은가?”**  
  - **A:** 원칙적으로 동일하지만, 실제 구현에서는 **미니배치**와 **온라인 학습**을 구분한다. 온라인 학습은 데이터가 실시간 스트림 형태로 들어올 때 사용된다.

- **Q4. “배치 크기가 모델 성능에 직접적인 영향을 미치는가?”**  
  - **A:** 배치 크기는 **일반화 성능**에 영향을 주는 하이퍼파라미터다. 일반적으로 **작은 배치**는 더 높은 테스트 정확도를 보이는 경향이 있지만, **훈련 시간**은 길어진다.

---

## 참고 문헌

[^1]: Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). *Imagenet classification with deep convolutional neural networks*. NIPS. https://doi.org/10.5555/2999325.2999455  
[^2]: Goyal, P., et al. (2017). *Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour*. arXiv:1706.02677. https://arxiv.org/abs/1706.02677  
[^3]: Hoffer, E., et al. (2017). *Train longer, generalize better: closing the generalization gap in large batch training of neural networks*. NIPS. https://arxiv.org/abs/1706.02677  
[^4]: You, Y., et al. (2020). *Large Batch Training of Convolutional Networks*. ICLR. https://arxiv.org/abs/1903.05379  
[^5]: Smith, L. N. (2017). *Cyclical Learning Rates for Training Neural Networks*. IEEE WACV. https://arxiv.org/abs/1506.01186  

--- 

*이 문서는 GitHub Flavored Markdown(GFM) 형식으로 작성되었으며, 위키 페이지로 바로 복사·붙여넣기하여 사용 가능합니다.*