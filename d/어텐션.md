# 어텐션(Attention)

## 개요
어텐션(Attention)은 **입력 데이터 중에서 현재 작업에 가장 중요한 부분에 가중치를 부여**하여 정보를 선택적으로 활용하는 메커니즘이다. 자연어 처리(NLP), 컴퓨터 비전(CV), 음성 처리, 강화학습 등 다양한 AI 분야에서 핵심 구성 요소로 자리 잡고 있으며, 특히 **Transformer** 모델의 탄생으로 인해 “Attention is All You Need”라는 슬로건처럼 많은 연구와 실용 시스템에 적용되고 있다.

## 배경 및 동기
전통적인 순환 신경망(RNN)이나 합성곱 신경망(CNN)은 입력 시퀀스를 고정된 방식으로 처리한다. 긴 시퀀스에서는 **장기 의존성**을 학습하기 어려워 *기억 손실* 문제가 발생한다. 2014년 Bahdanau와 동료들이 제안한 **Additive Attention**은 인코더-디코더 구조에서 디코더가 현재 번역에 필요한 인코더의 숨겨진 상태를 동적으로 선택할 수 있게 함으로써, 긴 문맥을 효과적으로 활용할 수 있게 만들었다[[1]](#ref1).

## 어텐션 메커니즘의 원리
어텐션은 일반적으로 **쿼리(Query)**, **키(Key)**, **밸류(Value)** 세 가지 벡터를 이용한다.

1. **쿼리(Q)** – 현재 단계에서 찾고자 하는 정보.
2. **키(K)** – 입력 전체에 대한 특징 표현.
3. **밸류(V)** – 실제 전달될 정보.

각 쿼리와 모든 키 사이의 유사도를 계산하여 **어텐션 스코어**를 얻고, 이를 정규화(보통 Softmax)하여 **가중치**를 만든다. 마지막으로 가중치를 밸류에 곱해 **컨텍스트 벡터**를 생성한다.

\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]

* \(d_k\) : 키 벡터 차원(스케일링을 위한 정규화 항).

위 식은 **Scaled Dot-Product Attention**이라 부르며, 가장 널리 사용되는 형태이다[[3]](#ref3).

## 어텐션 유형

### Soft Attention와 Hard Attention
- **Soft Attention** : 모든 키에 대해 연속적인 확률 분포를 부여한다. 미분 가능해 역전파가 가능하므로 대부분의 딥러닝 프레임워크에서 사용한다.
- **Hard Attention** : 특정 위치를 샘플링하거나 이진 마스크를 적용한다. 비미분 가능하지만 강화학습 기법(REINFORCE 등)이나 근사 기법을 통해 학습한다.

### Additive Attention (Bahdanau Attention)
쿼리와 키를 각각 선형 변환 후 **tanh** 활성화 함수를 적용하고, 가중치를 학습한다. 연산 복잡도는 \(O(N \cdot d)\)이며, 입력 길이에 비해 비교적 효율적이다[[1]](#ref1).

### Multiplicative Attention (Luong Attention)
두 벡터의 내적을 직접 사용하거나 선형 변환을 거친 후 정규화한다. **Scaled Dot-Product**와 동일한 형태이지만 스케일링 없이 적용하기도 한다[[2]](#ref2).

### Scaled Dot-Product Attention
위에서 제시한 공식과 같이, 점곱 연산에 \(\sqrt{d_k}\) 로 나누어 스케일링한다. 키 차원이 커질수록 점곱 값이 커지는 문제를 방지한다. Transformer의 기본 어텐션이다[[3]](#ref3).

### Multi-Head Attention
어텐션을 **여러 개의 헤드**(head)로 병렬 수행해 다양한 표현 공간을 동시에 학습한다. 각 헤드는 독립적인 쿼리·키·밸류 투영을 거치며, 최종 출력은 헤드들의 결과를 연결(concatenate) 후 선형 변환한다. 이는 모델이 **다양한 관계와 패턴**을 포착하도록 돕는다[[3]](#ref3).

## 자기 어텐션(Self-Attention)와 트랜스포머
**Self-Attention**은 입력 시퀀스 자체를 쿼리·키·밸류 모두로 사용하는 어텐션이다. 즉, 시퀀스 내의 각 토큰이 다른 모든 토큰과의 관계를 직접 학습한다. 이 메커니즘은 다음과 같은 장점을 제공한다.

- **병렬 처리**: 순차적 연산이 필요 없어 GPU/TPU에서 효율적인 병렬화가 가능.
- **전역 맥락**: 모든 토큰 간의 직접적인 관계를 모델링, 장기 의존성 문제 완화.
- **유연성**: 입력 길이에 따라 가변적인 구조가 가능.

### Transformer 구조
Transformer는 **Encoder‑Decoder** 구조를 갖춘 모델로, Encoder와 Decoder 각각에 **Multi-Head Self-Attention**과 **Feed‑Forward Network(FFN)** 블록을 쌓는다. 주요 컴포넌트는 다음과 같다.

| 구성 요소 | 설명 |
| -------- | ---- |
| **Multi-Head Self-Attention** | 입력 시퀀스 내 모든 토큰 간 관계 학습 |
| **Add & Norm** | Residual 연결 후 Layer Normalization 적용 |
| **Position‑wise FFN** | 두 개의 선형 변환과 ReLU로 구성된 비선형 변환 |
| **Positional Encoding** | 순서 정보를 삽입 (사인·코사인 함수를 이용) |

전체적인 연산 복잡도는 **\(O(N^2 \cdot d)\)** (N: 시퀀스 길이, d: 차원)이며, 이는 RNN 기반 모델에 비해 더 높은 메모리 요구량을 갖지만, 병렬 처리 효율성으로 전체 학습 속도가 크게 개선된다[[3]](#ref3).

## 어텐션 기반 모델들의 주요 구조

### Transformer
원 논문 “Attention is All You Need”에서 제안된 Transformer는 현재 가장 널리 쓰이는 어텐션 기반 아키텍처다. BERT, GPT, T5 등 다양한 파생 모델이 이 구조를 기반으로 한다.

### BERT와 GPT 시리즈
- **BERT (Bidirectional Encoder Representations from Transformers)**: Encoder만 사용해 양방향 컨텍스트를 학습한다. 마스크드 언어 모델링(Masked LM)과 Next Sentence Prediction(NSP) 목표를 이용해 사전 학습한다.
- **GPT (Generative Pre‑trained Transformer)**: Decoder만 사용해 순방향(왼→오른) 문맥을 학습한다. 언어 생성에 특화돼, 대규모 사전 학습 후 파인튜닝 없이도 다양한 작업에 바로 적용 가능하다[[3]](#ref3).

### Vision Transformer (ViT) 등
이미지를 **패치(patch)** 단위로 나누어 토큰화하고, Transformer에 입력한다. 기존 CNN 기반 비전 모델 대비 **전역적인 시각 관계**를 자연스럽게 파악한다. Dosovitskiy 등은 2020년 ViT를 제안해 ImageNet에서 CNN과 동등하거나 뛰어난 성능을 보였다[[4]](#ref4).

## 다양한 분야에서의 활용

### 자연어 처리(NLP)
- **기계 번역**: 어텐션은 번역 시 원문 각 단어와 번역문 단어 사이의 정렬을 자동 학습한다.
- **질문‑응답**: 문맥에 따라 답변을 추출하거나 생성하는 데 중요한 역할을 한다.
- **텍스트 요약**: 긴 문서를 핵심 문장에 집중하여 압축한다.
- **감정 분석·텍스트 분류**: 중요한 단어나 구절에 가중치를 부여해 성능 향상.

### 컴퓨터 비전(CV)
- **객체 검출**: DETR(Detection Transformer)은 어텐션을 이용해 객체와 위치를 직접 예측한다.
- **이미지 캡셔닝**: 이미지 특징과 텍스트 어텐션을 결합해 자연스러운 설명 문장을 생성한다.
- **이미지 분할**: SegFormer 등은 어텐션을 이용해 다중 스케일 정보를 효과적으로 통합한다.

### 음성 인식 및 합성
- **음성‑텍스트 변환**: Transformer‑based 모델은 시계열 특성을 어텐션으로 직접 학습한다.
- **텍스트‑음성 합성(TTS)**: Tacotron‑2와 같은 모델은 어텐션을 활용해 텍스트와 멜‑스펙트로그램을 정렬한다.

### 강화학습(RL)
- **마스크드 어텐션**: 에이전트가 환경 내 중요한 요소에 집중하도록 설계된다.
- **Transformer‑based 정책 네트워크**: 복잡한 시퀀스 의사결정 문제에서 전역적인 상태 정보를 효율적으로 활용한다.

## 장점과 한계

### 장점
- **전역적인 의존성 학습**: 입력 전체에 대해 직접적인 연결을 형성한다.
- **병렬화 용이**: 순차적 연산이 없으므로 하드웨어 가속 효율이 높다.
- **해석 가능성**: 어텐션 가중치를 시각화해 모델이 어느 부분에 집중했는지 확인 가능.
- **다양한 도메인 확장성**: NLP뿐만 아니라 CV, 음성, RL 등 여러 분야에 쉽게 적용 가능.

### 한계
- **연산·메모리 비용**: 시퀀스 길이가 \(N\)일 때 \(O(N^2)\) 복잡도는 긴 입력에 비효율적이다.
- **길이 제한**: 고정된 최대 길이(예: 512 토큰)보다 긴 시퀀스는 슬라이딩 윈도우나 길이 압축 기법이 필요하다.
- **비정형 관계 잡기 어려움**: 구조적(예: 트리) 정보를 직접 반영하기는 어렵다.
- **과적합 위험**: 대규모 파라미터와 데이터가 필요해 작은 데이터셋에서는 과적합이 쉽게 발생한다.

> **Tip**: 최근 연구에서는 **Linear Attention**, **Sparse Attention**, **Sliding‑Window Attention** 등 연산 효율성을 개선한 변형이 활발히 제안되고 있다[[5]](#ref5).

## 최신 연구 동향 및 향후 과제
1. **효율적인 어텐션**  
   - *Sparse Transformer*, *Longformer*, *Reformer* 등은 **희소성을 활용**하거나 **축소된 토큰 수**로 연산량을 크게 낮춘다.  
   - *Linear Attention*(예: Performer)은 커널 근사를 통해 \(O(N)\) 복잡도로 변환한다.

2. **멀티모달 어텐션**  
   - 텍스트·이미지·음성 등 서로 다른 모달리티를 **공통 어텐션 공간**에 매핑해 통합 이해를 목표한다.  
   - CLIP, Flamingo, GIT 등은 멀티모달 대규모 사전 학습 모델의 대표적인 사례다.

3. **어텐션 해석 및 신뢰성**  
   - 어텐션 가중치가 실제 모델 의사결정에 어떤 영향을 주는지 **이론적 분석**과 **시각화 도구**가 발전하고 있다.  
   - *Explainable AI*와의 연계 연구가 활발히 진행 중이다.

4. **생성 모델과 어텐션**  
   - Diffusion 모델과 어텐션을 결합해 고해상도 이미지·동영상 생성이 가능해졌다.  
   - 대형 언어 모델(LLaMA, GPT‑4 등)에서도 **리트리벌 기반 어텐션**(RAG)과 **플러그인 구조**가 도입되고 있다.

5. **계산 및 에너지 효율**  
   - 대규모 모델 학습에 따른 **탄소 발자국** 문제가 대두되면서, **지능형 프루닝**·**지식 증류**·**하드웨어 최적화**가 어텐션 기반 모델에 적용되고 있다.

> **예측**: 2020년대 후반까지는 **어텐션 기반 대규모 사전 학습 모델**이 AI 분야의 표준이 될 가능성이 높으며, 효율성 개선과 멀티모달 통합이 핵심 연구 흐름을 이끌 것으로 기대된다.

## 참고 문헌
<a name="ref1"></a>
1. Bahdanau, D., Cho, K., & Bengio, Y. (2015). *Neural Machine Translation by Jointly Learning to Align and Translate*. arXiv:1409.0473.  
<a name="ref2"></a>
2. Luong, M.-T., Pham, H., & Manning, C. D. (2015). *Effective Approaches to Attention-based Neural Machine Translation*. EMNLP.  
<a name="ref3"></a>
3. Vaswani, A. et al. (2017). *Attention Is All You Need*. *Advances in Neural Information Processing Systems*, 30, 5998–6008.  
<a name="ref4"></a>
4. Dosovitskiy, A. et al. (2020). *An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale*. arXiv:2010.11929.  
<a name="ref5"></a>
5. Child, R. et al. (2019). *Generating Long Sequences with Sparse Transformers*. arXiv:1904.10509.  

---  

*이 문서는 최신 연구 동향을 반영하기 위해 지속적으로 업데이트될 예정이며, 어텐션 메커니즘에 대한 보다 깊은 이해를 위해 원 논문 및 최신 리뷰 논문을 참고하시기 바랍니다.*