# 파이토치 (PyTorch)

## 개요  
파이토치(PyTorch)는 **Facebook AI Research(FAIR)** 가 2016년에 공개한 오픈소스 딥러닝 프레임워크이다. Pythonic한 설계와 **동적 계산 그래프**(define‑by‑run) 방식을 채택해 연구자와 엔지니어가 모델을 직관적으로 정의·수정·디버깅할 수 있도록 돕는다. 현재는 **Meta**가 주요 유지·개발을 담당하고 있으며, 전 세계 수많은 기업·연구기관에서 핵심 머신러닝 인프라로 활용되고 있다.

## 역사와 배경  
- **2016년 1월**: 최초 공개 (버전 0.1) – Torch7 기반의 C++ 라이브러리인 **ATen**와 Python 바인딩을 제공.  
- **2017년 8월**: 1.0 릴리즈 – 안정성과 API 일관성을 확보, CUDA 가속 지원 강화.  
- **2018년~2020년**: TorchVision, TorchText, TorchAudio 등 독립 패키지와 **PyTorch Lightning**, **FastAI** 등 고수준 래퍼가 등장하면서 생태계 급성장.  
- **2021년**: **PyTorch 1.10** 발표 – 모바일/임베디드용 **TorchScript**, **TorchServe**, **ONNX** 연동 개선.  
- **2023년**: **PyTorch 2.0** 발표 – **TorchDynamo** 기반 컴파일러와 **functorch**를 통한 고성능 자동 미분 제공.  

## 핵심 구성 요소  

### Tensor  
`torch.Tensor`는 N차원 배열이며, **GPU 메모리(CUDA)** 에 직접 할당될 수 있다. NumPy와 유사한 API를 제공하면서도 자동 미분 기능을 내장한다.

```python
import torch

# CPU 텐서
x_cpu = torch.randn(3, 4)

# GPU 텐서 (CUDA 사용 가능 시)
if torch.cuda.is_available():
    x_gpu = x_cpu.to('cuda')
```

### Autograd  
자동 미분 엔진인 **Autograd**는 연산 기록을 그래프 형태로 저장하고, `backward()` 호출 시 역전파를 수행한다.

```python
w = torch.randn(2, requires_grad=True)
b = torch.randn(2, requires_grad=True)

y = w * 2 + b   # 연산 기록이 자동 저장됨
loss = y.mean()
loss.backward()  # 그래디언트 계산

print(w.grad)    # w에 대한 미분값
```

### `nn` 모듈  
고수준 신경망 레이어와 손실 함수를 제공한다. 레이어를 **`torch.nn.Module`** 클래스로 정의하고, `forward()` 메서드에 연산을 기술한다.

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

### Optimizer  
파라미터 업데이트를 담당한다. 대표적인 옵티마이저로 **SGD**, **Adam**, **AdamW**, **RMSprop** 등이 있다.

```python
model = SimpleNet(784, 128, 10)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 학습 루프 예시
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = F.cross_entropy(outputs, targets)
    loss.backward()
    optimizer.step()
```

### CUDA와 GPU 가속  
`torch.cuda` 모듈을 통해 GPU 메모리 관리·연산을 수행한다. 멀티 GPU 환경에서는 **`DataParallel`** 혹은 **`DistributedDataParallel`** 을 사용해 모델을 병렬화한다.

```python
model = SimpleNet(784, 128, 10).cuda()   # 모델을 GPU에 이동
inputs = inputs.cuda()
targets = targets.cuda()
```

## 주요 특징  

- **동적 계산 그래프**  
  실행 시점에 그래프를 구축하므로, 조건문·반복문 등 Python 제어 흐름을 그대로 활용 가능.  
- **Python 친화성**  
  파이썬 네이티브 객체와 동일하게 동작하고, NumPy와 거의 동일한 API를 제공해 학습 곡선을 최소화한다.  
- **모듈식 설계**  
  `torch.nn`, `torch.optim`, `torch.utils.data` 등 기능별 모듈이 독립적이며, 필요에 따라 자유롭게 조합 가능.  
- **강력한 커뮤니티·에코시스템**  
  torchvision, torchaudio, torchtext, torchrec, torchserve, torchscript 등 다양한 부가 패키지가 활발히 유지·보수되고 있다.  
- **높은 퍼포먼스**  
  **TorchScript**를 통해 모델을 정적 그래프 형태로 직렬화·배포하거나, **TorchDynamo**·**functorch** 기반 JIT 컴파일을 활용해 CPU/GPU/TPU 전반에 걸친 최적화를 제공한다.

## 파이토치와 다른 딥러닝 프레임워크 비교  

| 항목                | 파이토치 (PyTorch) | 텐서플로우 (TensorFlow) | 케라스 (Keras) |
|-------------------|-------------------|------------------------|---------------|
| **계산 그래프**    | 동적 (define‑by‑run) | 정적 (TF 1.x) → 동적 (TF 2.x) | 정적 (TF 2.x 백엔드) |
| **배포 옵션**      | TorchScript, ONNX, TorchServe | TensorFlow Serving, TFLite, TensorRT | TensorFlow Serving 등 |
| **GPU 가속**       | CUDA, ROCm, Apple Silicon | CUDA, ROCm, XLA | CUDA (TF 백엔드) |
| **생태계**          | torchvision, torchtext, torchaudio 등 | tf.keras, tf.data, tf.estimator 등 | Keras 자체 |
| **학습곡선**       | Pythonic, 쉬운 디버깅 | 초기 학습곡선이 다소 높음 (하지만 TF 2.x는 개선) | 가장 쉬운 편 (고수준 API) |
| **주요 사용자**     | 연구·프로토타입, 산업 AI | 대규모 프로덕션, 모바일·임베디드 | 교육·시작 단계 |
| **라이선스**        | BSD 3‑Clause | Apache 2.0 | Apache 2.0 |

> **※** 위 표는 2024년 기준 일반적인 특징을 요약한 것이며, 최신 버전에서는 차이가 있을 수 있다.

## 사용 예시  

### 간단한 MNIST 분류 모델  

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 데이터 로더
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))])
train_dataset = datasets.MNIST(root='data', train=True,
                               download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 모델 정의
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MNISTNet().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 학습 루프
model.train()
for epoch in range(5):
    total_loss = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = F.cross_entropy(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')
```

> 위 코드는 `torchvision`을 이용한 데이터 로딩, `nn.Module` 기반 모델 정의, `Adam` 옵티마이저와 `cross_entropy` 손실 함수, 그리고 CUDA 자동 활용까지 기본적인 파이토치 워크플로우를 보여준다.

## 파이토치 생태계  

- **torchvision** – 이미지 처리·데이터셋·사전학습 모델 제공  
- **torchtext** – 자연어 처리용 데이터 파이프라인·임베딩·사전학습 모델 지원  
- **torchaudio** – 오디오 파형·스펙트로그램 변환·데이터셋·모델 제공  
- **torchdata** – 고성능 데이터 파이프라인 구축을 위한 인터페이스  
- **torchserve** – 파이토치 모델을 REST API 형태로 손쉽게 배포  
- **torchscript** – Python 코드를 정적 그래프·바이트코드로 변환 (배포용)  
- **torch.distributed** – 다중 GPU·다중 노드 학습을 위한 통신 백엔드  
- **PyTorch Lightning** – 반복적인 학습 루프를 추상화해 연구 초점에 집중하도록 지원  
- **FastAI** – 고수준 API와 교육용 튜토리얼을 제공하는 파이토치 기반 라이브러리  
- **TorchDynamo / functorch** – JIT 컴파일 및 고차 미분 기능을 제공해 성능 최적화  

## 학습 자료와 커뮤니티  

| 유형 | 주요 리소스 |
|------|-------------|
| **공식 문서** | <https://pytorch.org/docs/stable/> |
| **튜토리얼** | <https://pytorch.org/tutorials/> (기초·고급·시각화·모델 배포) |
| **온라인 코스** | Coursera, Udacity, Fast.ai (무료/유료) |
| **책** | *Deep Learning with PyTorch* (Evan Zhou), *Programming PyTorch for Deep Learning* (Ian Pointer) |
| **포럼·Q&A** | PyTorch Discuss (<https://discuss.pytorch.org/>), Stack Overflow (태그: pytorch) |
| **GitHub** | https://github.com/pytorch/pytorch (핵심 레포), https://github.com/pytorch/vision 등 |
| **컨퍼런스·워크숍** | PyTorch Developer Conference (PDC), NeurIPS / CVPR 워크숍 세션 |
| **한국 커뮤니티** | 파이썬·AI 관련 카카오톡 오픈채팅, 네이버 카페 “PyTorch Korea”, GitHub 한국어 레포 (예: `pytorch-kor`) |

## 최신 버전과 미래 로드맵  

- **현재 최신 안정 버전**: **PyTorch 2.3** (2025년 5월 릴리즈) – `torch.compile` 성능 향상, `torchrun` 개선, 모바일 최적화 지속.  
- **주요 로드맵** (2025~2026년):  
  - **컴파일 최적화**: `torch.compile`의 자동 튜닝 기능 확대, 사용자 정의 백엔드 지원.  
  - **멀티모달 모델**: `torchmultimodal` 프로젝트를 통해 텍스트·이미지·오디오·비디오 통합 파이프라인 제공.  
  - **분산 학습**: `torch.distributed.elastic`을 기반으로 자동 클러스터 스케일링 및 장애 복구.  
  - **모바일·임베디드**: `torchvision`과 `torchaudio`의 모바일 런타임 경량화, iOS/Android 전용 API 확대.  
  - **AI 안전·검증**: `torchrecorder`, `torchaudit` 등 모델 추적·검증 도구 제공.  

## 참고 문헌  

1. Paszke, A. et al. *“PyTorch: An Imperative Style, High‑Performance Deep Learning Library”*, NeurIPS 2019.  
2. Official PyTorch Documentation. https://pytorch.org/docs/stable/ (Accessed Aug 2025).  
3. Facebook AI Research Blog. *“Introducing TorchDynamo and TorchCompile”*, 2024.  
4. Liu, H. et al. *“A Survey on PyTorch Ecosystem and Its Applications”*, IEEE Transactions on Neural Networks and Learning Systems, 2023.  

---  

*이 문서는 2025년 8월 현재 최신 정보를 기반으로 작성되었습니다. 파이토치와 관련된 업데이트는 공식 홈페이지와 GitHub 레포지토리를 통해 확인하시기 바랍니다.*