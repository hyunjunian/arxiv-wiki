논문 제목: Attention Is All You Need

핵심 내용 요약:

1. 문제 제기 및 배경
- 기존 순차 변환(sequence transduction) 모델들은 주로 복잡한 순환 신경망(RNN)이나 컨볼루션 신경망(CNN)을 기반으로 하며, 인코더와 디코더 구조를 포함한다.
- 최고 성능의 모델들은 인코더와 디코더 간에 '어텐션(attention)' 메커니즘을 연계하여 사용한다.
- 순환 신경망은 내부적으로 시퀀스 길이에 따라 연산을 순차적으로 처리해야 하므로 병렬 처리에 한계가 있다.

2. 제안하는 모델: Transformer
- Transformer는 재발(recurrence)이나 컨볼루션을 전혀 사용하지 않고 오직 어텐션(특히 'self-attention') 메커니즘만으로 구성된 새로운 네트워크 아키텍처이다.
- 인코더와 디코더 모두 여러 층으로 쌓인 self-attention과 포인트별 완전 연결 피드포워드 네트워크로 구성됨.
- 어텐션은 query, key, value 벡터간의 scaled dot-product 형태로 계산되며, 여러 어텐션 헤드를 병렬로 사용하는 multi-head attention을 적용함으로써 다양한 시각(subspace)에서 정보를 동시에 취급할 수 있다.
- 위치 정보를 표현하기 위해 별도의 positional encoding(사인/코사인 함수)도 삽입하여 순서 정보를 모델에 부여함.

3. Transformer의 장점
- 어텐션 층은 입력 시퀀스 길이에 대해 O(1)의 순차 연산 단계를 갖고 있어 병렬 처리가 쉬움 (반면 RNN은 O(n), CNN은 O(1)이나 더 긴 최대 경로 길이 필요).
- 긴 거리의 의존성에 대하여 짧은 경로를 제공하여 학습이 용이.
- 기존 모델 대비 학습 시간이 대폭 단축되면서도 번역 품질(BLEU 점수)이 크게 향상됨.

4. 실험 및 결과
- WMT 2014 영어-독일어 번역에서 Transformer (big) 모델이 BLEU 28.4로 이전 최고 모델 및 앙상블 모델을 뛰어넘음.
- 영어-프랑스어 번역에서도 새로운 최고 점수 기록(41.8 BLEU).
- 적은 학습 비용과 병렬화 효율을 보임.
- 영어 문법 분석(구문분석) 태스크에서도 좋은 성능을 내어 Transformer가 번역 외 시퀀스 처리 문제에 일반화 가능함을 증명.

5. 결론
- Transformer는 순환이나 컨볼루션 계층 없이 오로지 어텐션 메커니즘으로만 구성된 최초의 시퀀스 변환 네트워크임.
- 효율성, 성능, 병렬 처리 면에서 우수하여 향후 다양한 분야(텍스트 이외의 이미지, 오디오, 비디오)에도 확장 가능성을 제시함.

요약하자면, 이 논문은 기존 RNN/CNN 기반 모델들을 대신하여 '어텐션'만으로 동작하는 Transformer 모델을 제안하여, 학습 효율과 성능을 크게 향상시키고 이를 통해 자연어 처리 분야에 혁신을 가져왔다는 점이 핵심입니다.